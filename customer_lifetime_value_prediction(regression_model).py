# -*- coding: utf-8 -*-
"""Customer Lifetime Value Prediction(Regression model).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pe87WLV8TQLXYQvshuQYAlnTJaePi_b6

## Customer Lifetime Value Prediction using Regression model
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV

import matplotlib.pyplot as plt
import seaborn as sns

#Read data
cus_data = pd.read_csv('/content/customer_segmentation.csv', encoding='cp1252')

cus_data.head()

# Get the number of rows in the DataFrame
num_rows = len(cus_data)

# Print the number of rows
print("Number of rows in the dataset:", num_rows)

# Drop duplicate rows based on all columns
cus_data = cus_data.drop_duplicates()

# Get the number of rows in the DataFrame after dropping duplicates
num_rows_after_dropping = len(cus_data)

# Print the number of rows after dropping duplicates
print("Number of rows in the dataset after dropping duplicates:", num_rows_after_dropping)

# Plotting the number of customers from each country
country_counts = cus_data['Country'].value_counts()
plt.figure(figsize=(12, 6))
country_counts.plot(kind='bar')
plt.title('Number of Customers per Country')
plt.xlabel('Country')
plt.ylabel('Number of Customers')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.show()

#we will be using only UK data
cus_uk = cus_data.query("Country=='United Kingdom'").reset_index(drop=True)

cus_uk.head()

#create a generic user dataframe to keep CustomerID and new segmentation scores
df_user = pd.DataFrame(cus_data['CustomerID'].unique())
df_user.columns = ['CustomerID']
df_user.head()

df_user['CustomerID'].plot(kind='line', figsize=(8, 4), title='CustomerID')
plt.gca().spines[['top', 'right']].set_visible(False)

cus_data.describe()

# Filter returns/adjustments
returns_df = cus_data[(cus_data['Quantity'] < 0) | (cus_data['UnitPrice'] < 0)]

# Keep positive values for revenue analysis
uk_data = cus_data[(cus_data['Quantity'] > 0) & (cus_data['UnitPrice'] > 0)]

uk_data.describe()

"""# Calculate RFM Metrics:
Recency: Days since the customer's last purchase.

Frequency: Count of unique invoices per customer.

Monetary: Total revenue per customer (Quantity Ã— UnitPrice).
"""

# use only required columns for CLTV
uk_data = uk_data[['CustomerID', 'InvoiceDate', 'InvoiceNo', 'Quantity', 'UnitPrice']]

# calculate total purchase
uk_data['TotalPurchase'] = uk_data['Quantity'] * uk_data['UnitPrice']

uk_data['InvoiceDate'] = pd.to_datetime(uk_data['InvoiceDate'])

# perform aggregate operations
rfm_df = uk_data.groupby('CustomerID').agg({
    'InvoiceDate': lambda date: (date.max() - date.min()).days, # Recency
    'InvoiceNo': lambda num: len(num),   # Frequency
    'Quantity': lambda quantity: quantity.sum(),
    'TotalPurchase': lambda total_pur: total_pur.sum()
}).reset_index()

rfm_df.columns = ['CustomerID', 'Recency', 'Frequency','Quantity', 'Monetary']

rfm_df.head()

#  CustomerID vs Frequency

def _plot_series(series, series_name, series_index=0):
  palette = list(sns.palettes.mpl_palette('Dark2'))
  xs = series['CustomerID']
  ys = series['Frequency']

  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])

fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')
df_sorted = rfm_df.sort_values('CustomerID', ascending=True)
_plot_series(df_sorted, '')
sns.despine(fig=fig, ax=ax)
plt.xlabel('CustomerID')
_ = plt.ylabel('Frequency')

#  Monetary Value by Recency and Frequency

plt.figure(figsize=(10, 6))
plt.scatter(rfm_df['Recency'], rfm_df['Frequency'], s=rfm_df['Monetary'] / 100, alpha=0.5, color='green')
plt.title('Monetary Value by Recency and Frequency')
plt.xlabel('Recency')
_ = plt.ylabel('Frequency')

rfm_df['avg_order_val'] = rfm_df['Monetary'] / rfm_df['Quantity']

purchase_frequency = rfm_df['Frequency'].nunique() / rfm_df.shape[0]

repeat_rate=rfm_df[rfm_df['Frequency'] > 1].shape[0]/rfm_df.shape[0]

churn_rate = 1 - repeat_rate

print(f"Purchase Frequency: {purchase_frequency}")
print(f"Repeat Rate: {repeat_rate}")
print(f"Churn Rate: {churn_rate}")

# Calculate Profit margin assuming gain of 5%
rfm_df['profit_margin'] = rfm_df['Monetary'] * 0.05

rfm_df['cust_lifetime_value'] = ((rfm_df['avg_order_val'] * purchase_frequency)/churn_rate) * \
                                     rfm_df['profit_margin']

rfm_df.sample(n=10)

uk_data['month_yr'] = uk_data['InvoiceDate'].apply(lambda x: x.strftime('%b-%Y'))

sale_df = uk_data.pivot_table(index=['CustomerID'], columns=['month_yr'], values='TotalPurchase', aggfunc='sum', fill_value=0).reset_index()
sale_df['CLV'] = sale_df.iloc[:, 2:].sum(axis=1)

sale_df.sample(n=10)

sale_df['log_CLV'] = np.log1p(sale_df['CLV'])  # log1p handles zero values safely

# Before Transformation
plt.figure(figsize=(10, 5))
sns.histplot(sale_df['CLV'], bins=50, kde=True, color='blue')
plt.title('CLV Distribution Before Transformation')
plt.show()

# After Log Transformation
plt.figure(figsize=(10, 5))
sns.histplot(sale_df['log_CLV'], bins=50, kde=True, color='green')
plt.title('CLV Distribution After Log Transformation')
plt.show()

sale_df.sample(n=10)

# Define custom percentile thresholds
percentiles = [0, 0.5, 0.8, 0.95, 1]
labels = ['Low', 'Medium', 'High', 'Top']

# Compute thresholds for CLV
thresholds = rfm_df['cust_lifetime_value'].quantile(percentiles).values

# Create segments
rfm_df['CLV_segment'] = pd.cut(rfm_df['cust_lifetime_value'], bins=thresholds, labels=labels, include_lowest=True)

# Check distribution
print(rfm_df['CLV_segment'].value_counts())

rfm_df['CLV_segment'].value_counts().plot.pie(
    autopct='%1.1f%%', figsize=(8, 8), title="Customer Segmentation by CLV", startangle=90, colors=['#ff9999','#66b3ff','#99ff99','#ffcc99']
)
plt.ylabel('')
plt.show()

X=sale_df[['Dec-2011','Nov-2011', 'Oct-2011','Sep-2011','Aug-2011','Jul-2011','Jun-2011','May-2011', 'Apr-2011','Mar-2011','Feb-2011','Jan-2011','Dec-2010']]
y=sale_df[['log_CLV']]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

"""# Linear Regression"""

# Train and predict the model
LR = LinearRegression()
LR.fit(X_train, y_train)

y_pred_score = LR.score(X_test, y_test)
print("Prediction score is: {}".format(y_pred_score))

y_pred = LR.predict(X_test)

# Check R-Squared value
print("R-Squared: {}".format(metrics.r2_score(y_test, y_pred)))
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

sns.scatterplot(x=y_test.values.flatten(), y=y_pred.flatten())
plt.xlabel("Actual CLV")
plt.ylabel("Predicted CLV")
plt.title("Actual vs Predicted CLV (Linear Regression)")
plt.show()

"""# Gradient Boosting Regressor"""

# Initialize Gradient Boosting Regressor
gb = GradientBoostingRegressor(random_state=42)

# Define the hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],  # Number of boosting stages
    'learning_rate': [0.01, 0.05, 0.1],  # Shrinks the contribution of each tree
    'max_depth': [3, 5, 7],  # Maximum depth of the individual trees
    'min_samples_split': [2, 5, 10],  # Minimum samples required to split
    'min_samples_leaf': [1, 2, 4],  # Minimum samples required at a leaf node
    'subsample': [0.7, 0.9, 1.0]  # Fraction of samples used for fitting individual trees
}

# Set up GridSearchCV
grid_search = GridSearchCV(estimator=gb, param_grid=param_grid,
                           cv=3, scoring='r2', verbose=2, n_jobs=-1)

# Fit to the training data
grid_search.fit(X_train, y_train)

# Output the best parameters and score
print("Best Parameters:", grid_search.best_params_)
print("Best R-Squared on Training Data:", grid_search.best_score_)

# Test the tuned model on the test set
best_gb = grid_search.best_estimator_
y_pred = best_gb.predict(X_test)

# Evaluate the tuned model
from sklearn.metrics import mean_squared_error, r2_score

print("R-Squared on Test Data:", r2_score(y_test, y_pred))
print("Mean Squared Error on Test Data:", mean_squared_error(y_test, y_pred))

feature_importances = best_gb.feature_importances_
indices = np.argsort(feature_importances)

plt.figure(figsize=(10, 6))
plt.barh(range(X_train.shape[1]), feature_importances[indices], align="center")
plt.yticks(range(X_train.shape[1]), X_train.columns[indices])
plt.title("Feature Importance")
plt.show()

# Create scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.6, color='blue', label='Predictions')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', linewidth=2, label='Perfect Prediction Line')

# Add labels and title
plt.title('Actual vs Predicted CLV (Gradient Boosting)', fontsize=14)
plt.xlabel('Actual CLV', fontsize=12)
plt.ylabel('Predicted CLV', fontsize=12)
plt.legend(fontsize=10)
plt.grid(alpha=0.3)
plt.show()